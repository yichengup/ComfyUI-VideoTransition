"""
è§†é¢‘é€æ˜å åŒ–è½¬åœºèŠ‚ç‚¹
ä¸¤ä¸ªè§†é¢‘ç‰‡æ®µä¹‹é—´çš„ç®€å•é€æ˜å åŒ–è½¬åœºæ•ˆæœ
"""

import torch
import numpy as np
import cv2
from comfy.comfy_types.node_typing import ComfyNodeABC, InputTypeDict, IO


class VideoCrossfadeNode(ComfyNodeABC):
    """è§†é¢‘é€æ˜å åŒ–è½¬åœº - ä¸¤ä¸ªè§†é¢‘ä¹‹é—´çš„å¤šç§è½¬åœºæ¨¡å¼"""
    
    CATEGORY = "VideoTransition"
    
    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "video1": (IO.IMAGE,),  # ç¬¬ä¸€ä¸ªè§†é¢‘
                "video2": (IO.IMAGE,),  # ç¬¬äºŒä¸ªè§†é¢‘
                "transition_mode": ([
                    "crossfade",           # ç›´æ¥å åŒ–
                    "fade_to_black",       # é—ªé»‘
                    "fade_to_white",       # é—ªç™½
                    "fade_to_custom",      # é—ªè‡ªå®šä¹‰è‰²
                    "additive_dissolve",   # å åŠ 
                    "chromatic_dissolve",  # è‰²æ•£å åŒ–
                ],),
                "total_frames": (IO.INT, {"default": 30, "min": 1, "max": 300}),
                "fps": (IO.INT, {"default": 30, "min": 15, "max": 60}),
            },
            "optional": {
                "background_color": (IO.STRING, {"default": "#FF0000"}),  # ç”¨äº fade_to_custom æ¨¡å¼ï¼Œé»˜è®¤çº¢è‰²
                "width": (IO.INT, {"default": 640, "min": 64, "max": 3840}),
                "height": (IO.INT, {"default": 640, "min": 64, "max": 2160}),
            }
        }
    
    RETURN_TYPES = (IO.IMAGE,)
    RETURN_NAMES = ("video",)
    FUNCTION = "generate_crossfade"
    
    async def generate_crossfade(
        self, 
        video1, 
        video2,
        transition_mode,
        total_frames, 
        fps,
        background_color="#000000",
        width=640,
        height=640
    ):
        """ç”Ÿæˆè§†é¢‘è½¬åœºæ•ˆæœ"""
        
        # æå–è§†é¢‘å¸§
        frames1 = self._extract_video_frames(video1)
        frames2 = self._extract_video_frames(video2)
        
        print(f"Video crossfade transition: {len(frames1)} + {len(frames2)} frames -> {total_frames} frames")
        print(f"Mode: {transition_mode}")
        
        # è§£æèƒŒæ™¯é¢œè‰²
        bg_color_bgr = self._parse_background_color(background_color)
        
        # ä½¿ç”¨OpenCVç›´æ¥åˆæˆï¼ˆç®€å•é«˜æ•ˆï¼‰
        output_frames = []
        
        for i in range(total_frames):
            progress = i / (total_frames - 1) if total_frames > 1 else 0
            
            # è·å–å¯¹åº”çš„å¸§ - æ ¹æ®è¿›åº¦è·å–
            frame1_idx = min(int(progress * (len(frames1) - 1)), len(frames1) - 1)
            frame2_idx = min(int(progress * (len(frames2) - 1)), len(frames2) - 1)
            
            frame1 = frames1[frame1_idx]
            frame2 = frames2[frame2_idx]
            
            # æ ¹æ®è½¬åœºæ¨¡å¼è¿›è¡Œä¸åŒçš„æ··åˆ
            if transition_mode == "crossfade":
                # ç›´æ¥å åŒ–
                blended_frame = self._blend_frames_with_background(
                    frame1, frame2, progress, bg_color_bgr, width, height
                )
            elif transition_mode == "fade_to_black":
                # é—ªé»‘ï¼švideo1 â†’ é»‘è‰² â†’ video2
                blended_frame = self._fade_through_color(
                    frame1, frame2, progress, (0, 0, 0), width, height
                )
            elif transition_mode == "fade_to_white":
                # é—ªç™½ï¼švideo1 â†’ ç™½è‰² â†’ video2
                blended_frame = self._fade_through_color(
                    frame1, frame2, progress, (255, 255, 255), width, height
                )
            elif transition_mode == "fade_to_custom":
                # é—ªè‡ªå®šä¹‰è‰²ï¼švideo1 â†’ è‡ªå®šä¹‰é¢œè‰² â†’ video2
                blended_frame = self._fade_through_color(
                    frame1, frame2, progress, bg_color_bgr, width, height
                )
            elif transition_mode == "additive_dissolve":
                # å åŠ ï¼šä½¿ç”¨åŠ æ³•æ··åˆ
                blended_frame = self._additive_blend(
                    frame1, frame2, progress, width, height
                )
            elif transition_mode == "chromatic_dissolve":
                # è‰²æ•£å åŒ–ï¼šRGBé€šé“é”™ä½
                blended_frame = self._chromatic_blend(
                    frame1, frame2, progress, width, height
                )
            else:
                # é»˜è®¤ä½¿ç”¨ç›´æ¥å åŒ–
                blended_frame = self._blend_frames_with_background(
                    frame1, frame2, progress, bg_color_bgr, width, height
                )
            
            output_frames.append(blended_frame)
            
            # æ˜¾ç¤ºè¿›åº¦ - æ¯20å¸§æˆ–å®Œæˆæ—¶æ˜¾ç¤º
            if (i + 1) % 20 == 0 or i == total_frames - 1:
                progress_percent = (i + 1) / total_frames * 100
                print(f"Processing frames {i+1}/{total_frames} ({progress_percent:.1f}%)")
        
        # è½¬æ¢ä¸ºtensor
        video_tensor = self._frames_to_tensor(output_frames)
        
        print(f"Crossfade transition completed: {video_tensor.shape}")
        return (video_tensor,)
    
    def _extract_video_frames(self, video_tensor):
        """ä»è§†é¢‘tensorä¸­æå–å¸§"""
        frames = []
        
        # è§†é¢‘tensoræ ¼å¼: [B, H, W, C]
        if video_tensor.dim() == 4:
            batch_size = video_tensor.shape[0]
            for i in range(batch_size):
                frame = video_tensor[i]  # [H, W, C]
                frames.append(frame)
        else:
            # å•å¸§æƒ…å†µ
            frames.append(video_tensor)
        
        return frames
    
    def _parse_background_color(self, color_str):
        """è§£æèƒŒæ™¯é¢œè‰²å­—ç¬¦ä¸²ä¸ºBGRå…ƒç»„"""
        if color_str.startswith('#'):
            # ç§»é™¤#å·
            hex_color = color_str[1:]
            # è½¬æ¢ä¸ºRGB
            r = int(hex_color[0:2], 16)
            g = int(hex_color[2:4], 16)
            b = int(hex_color[4:6], 16)
            # è¿”å›BGRæ ¼å¼ï¼ˆOpenCVä½¿ç”¨BGRï¼‰
            return (b, g, r)
        else:
            # é»˜è®¤é»‘è‰²
            return (0, 0, 0)
    
    def _fade_through_color(self, frame1, frame2, progress, color_bgr, width, height):
        """é—ªè‰²è½¬åœºï¼šframe1 â†’ çº¯è‰² â†’ frame2"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(frame1, torch.Tensor):
            frame1 = frame1.cpu().numpy()
        if isinstance(frame2, torch.Tensor):
            frame2 = frame2.cpu().numpy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        if frame1.dtype == np.float32 or frame1.dtype == np.float64:
            frame1 = (frame1 * 255).clip(0, 255).astype(np.uint8)
        if frame2.dtype == np.float32 or frame2.dtype == np.float64:
            frame2 = (frame2 * 255).clip(0, 255).astype(np.uint8)
        
        # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
        frame1_resized = cv2.resize(frame1, (width, height), interpolation=cv2.INTER_AREA)
        frame2_resized = cv2.resize(frame2, (width, height), interpolation=cv2.INTER_AREA)
        
        # åˆ›å»ºçº¯è‰²ç”»å¸ƒ
        color_canvas = np.full((height, width, 3), color_bgr, dtype=np.uint8)
        
        # åˆ†ä¸¤ä¸ªé˜¶æ®µï¼š
        # é˜¶æ®µ1ï¼ˆ0 -> 0.5ï¼‰ï¼šframe1 æ·¡å‡ºåˆ°çº¯è‰²
        # é˜¶æ®µ2ï¼ˆ0.5 -> 1.0ï¼‰ï¼šçº¯è‰²æ·¡å…¥åˆ° frame2
        if progress < 0.5:
            # é˜¶æ®µ1ï¼šframe1 â†’ çº¯è‰²
            fade_progress = progress * 2  # æ˜ å°„åˆ° 0-1
            alpha1 = 1.0 - fade_progress
            alpha2 = fade_progress
            blended = cv2.addWeighted(frame1_resized, alpha1, color_canvas, alpha2, 0)
        else:
            # é˜¶æ®µ2ï¼šçº¯è‰² â†’ frame2
            fade_progress = (progress - 0.5) * 2  # æ˜ å°„åˆ° 0-1
            alpha1 = 1.0 - fade_progress
            alpha2 = fade_progress
            blended = cv2.addWeighted(color_canvas, alpha1, frame2_resized, alpha2, 0)
        
        return blended
    
    def _blend_frames_with_background(self, frame1, frame2, progress, bg_color_bgr, width, height):
        """ä½¿ç”¨OpenCVåˆæˆä¸¤å¸§ï¼ŒåŒ…å«èƒŒæ™¯è‰²å¤„ç†"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(frame1, torch.Tensor):
            frame1 = frame1.cpu().numpy()
        if isinstance(frame2, torch.Tensor):
            frame2 = frame2.cpu().numpy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        if frame1.dtype == np.float32 or frame1.dtype == np.float64:
            frame1 = (frame1 * 255).clip(0, 255).astype(np.uint8)
        if frame2.dtype == np.float32 or frame2.dtype == np.float64:
            frame2 = (frame2 * 255).clip(0, 255).astype(np.uint8)
        
        # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
        frame1_resized = cv2.resize(frame1, (width, height), interpolation=cv2.INTER_AREA)
        frame2_resized = cv2.resize(frame2, (width, height), interpolation=cv2.INTER_AREA)
        
        # é€æ˜å åŒ–: frame1é€æ¸æ·¡å‡ºï¼Œframe2é€æ¸æ·¡å…¥
        alpha1 = 1.0 - progress  # frame1çš„é€æ˜åº¦
        alpha2 = progress        # frame2çš„é€æ˜åº¦
        
        # ä½¿ç”¨OpenCVçš„addWeightedè¿›è¡Œé€æ˜å åŒ–
        blended = cv2.addWeighted(frame1_resized, alpha1, frame2_resized, alpha2, 0)
        
        # åˆ›å»ºèƒŒæ™¯è‰²ç”»å¸ƒ
        background_canvas = np.full((height, width, 3), bg_color_bgr, dtype=np.uint8)
        
        # å¦‚æœè¾“å…¥å¸§æœ‰Alphaé€šé“ï¼Œè¿›è¡ŒAlphaåˆæˆ
        if len(blended.shape) == 3 and blended.shape[2] == 4:  # RGBA
            alpha_channel = blended[:, :, 3] / 255.0
            rgb_channels = blended[:, :, :3]
            
            # Alphaåˆæˆåˆ°èƒŒæ™¯
            for c in range(3):
                background_canvas[:, :, c] = (
                    background_canvas[:, :, c] * (1 - alpha_channel) + 
                    rgb_channels[:, :, c] * alpha_channel
                ).astype(np.uint8)
        else:  # RGBï¼Œç›´æ¥ä½¿ç”¨æ··åˆç»“æœ
            background_canvas = blended
        
        return background_canvas
    
    def _additive_blend(self, frame1, frame2, progress, width, height):
        """å åŠ æ··åˆï¼šä¸¤ä¸ªè§†é¢‘ç›´æ¥ç›¸åŠ ï¼Œäº§ç”Ÿæ›´äº®çš„å åŠ æ•ˆæœ"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(frame1, torch.Tensor):
            frame1 = frame1.cpu().numpy()
        if isinstance(frame2, torch.Tensor):
            frame2 = frame2.cpu().numpy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        if frame1.dtype == np.float32 or frame1.dtype == np.float64:
            frame1 = (frame1 * 255).clip(0, 255).astype(np.uint8)
        if frame2.dtype == np.float32 or frame2.dtype == np.float64:
            frame2 = (frame2 * 255).clip(0, 255).astype(np.uint8)
        
        # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
        frame1_resized = cv2.resize(frame1, (width, height), interpolation=cv2.INTER_AREA)
        frame2_resized = cv2.resize(frame2, (width, height), interpolation=cv2.INTER_AREA)
        
        # ğŸ¯ çœŸæ­£çš„å åŠ ï¼šä½¿ç”¨Screenæ··åˆæ¨¡å¼æˆ–å¢å¼ºçš„åŠ æ³•æ··åˆ
        # Screenæ¨¡å¼å…¬å¼: 1 - (1-A) * (1-B)
        # æ•ˆæœï¼šä¸¤ä¸ªè§†é¢‘ç›¸åŠ ä½†ä¸ä¼šè¿‡åº¦æ›å…‰ï¼Œäº§ç”Ÿå‘å…‰å åŠ æ•ˆæœ
        
        # æ–¹æ¡ˆï¼šåœ¨è½¬åœºä¸­é—´æ—¶ï¼Œä¸¤ä¸ªè§†é¢‘éƒ½ä¿æŒè¾ƒé«˜çš„å¯è§åº¦
        # ä½¿ç”¨æ›²çº¿è®©ä¸­é—´æ›´äº®
        if progress < 0.5:
            # å‰åŠæ®µï¼šframe1ä¿æŒæ»¡äº®åº¦ï¼Œframe2é€æ¸å¢å¼º
            alpha1 = 1.0
            alpha2 = progress * 2  # 0 -> 1
        else:
            # ååŠæ®µï¼šframe1é€æ¸å‡å¼±ï¼Œframe2ä¿æŒæ»¡äº®åº¦
            alpha1 = (1.0 - progress) * 2  # 1 -> 0
            alpha2 = 1.0
        
        # å…ˆè½¬æ¢ä¸ºfloatå¹¶å½’ä¸€åŒ–åˆ°0-1
        frame1_norm = frame1_resized.astype(np.float32) / 255.0
        frame2_norm = frame2_resized.astype(np.float32) / 255.0
        
        # ğŸ”¥ ä½¿ç”¨Screenæ··åˆæ¨¡å¼ï¼š1 - (1-A) * (1-B)
        # è¿™æ ·ä¸¤ä¸ªäº®çš„åŒºåŸŸå åŠ ä¼šæ›´äº®ï¼Œä½†ä¸ä¼šå®Œå…¨è¿‡æ›
        screen_blend = 1.0 - (1.0 - frame1_norm * alpha1) * (1.0 - frame2_norm * alpha2)
        
        # è½¬æ¢å›0-255
        blended = (screen_blend * 255.0).clip(0, 255).astype(np.uint8)
        
        return blended
    
    def _chromatic_blend(self, frame1, frame2, progress, width, height):
        """è‰²æ•£å åŒ–ï¼šRGBé€šé“é”™ä½æ··åˆï¼Œäº§ç”Ÿè‰²æ•£/è‰²å·®æ•ˆæœ"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(frame1, torch.Tensor):
            frame1 = frame1.cpu().numpy()
        if isinstance(frame2, torch.Tensor):
            frame2 = frame2.cpu().numpy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        if frame1.dtype == np.float32 or frame1.dtype == np.float64:
            frame1 = (frame1 * 255).clip(0, 255).astype(np.uint8)
        if frame2.dtype == np.float32 or frame2.dtype == np.float64:
            frame2 = (frame2 * 255).clip(0, 255).astype(np.uint8)
        
        # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
        frame1_resized = cv2.resize(frame1, (width, height), interpolation=cv2.INTER_AREA)
        frame2_resized = cv2.resize(frame2, (width, height), interpolation=cv2.INTER_AREA)
        
        # è®¡ç®—é€šé“åç§»é‡ï¼ˆåœ¨è½¬åœºä¸­é—´æœ€å¤§ï¼‰
        # ä½¿ç”¨sinæ›²çº¿ä½¿åç§»å¹³æ»‘
        max_offset = 10  # æœ€å¤§åç§»åƒç´ 
        offset_intensity = np.sin(progress * np.pi)  # 0 -> 1 -> 0
        offset = int(max_offset * offset_intensity)
        
        # åˆ›å»ºè¾“å‡ºå›¾åƒ
        blended = np.zeros_like(frame1_resized)
        
        # è®¡ç®—æ··åˆæƒé‡
        alpha1 = 1.0 - progress
        alpha2 = progress
        
        # åˆ†åˆ«å¤„ç†RGBä¸‰ä¸ªé€šé“ï¼Œæ¯ä¸ªé€šé“æ–½åŠ ä¸åŒçš„åç§»
        # Ré€šé“ï¼šå‘å³åç§»
        if offset > 0:
            # frame1çš„Ré€šé“å‘å³åç§»
            blended[:, offset:, 2] = (
                frame1_resized[:, :-offset, 2].astype(np.float32) * alpha1 +
                frame2_resized[:, offset:, 2].astype(np.float32) * alpha2
            ).astype(np.uint8)
            blended[:, :offset, 2] = (
                frame1_resized[:, :offset, 2].astype(np.float32) * alpha1 +
                frame2_resized[:, :offset, 2].astype(np.float32) * alpha2
            ).astype(np.uint8)
        else:
            blended[:, :, 2] = (
                frame1_resized[:, :, 2].astype(np.float32) * alpha1 +
                frame2_resized[:, :, 2].astype(np.float32) * alpha2
            ).astype(np.uint8)
        
        # Gé€šé“ï¼šä¸åç§»ï¼ˆä¸­å¿ƒï¼‰
        blended[:, :, 1] = (
            frame1_resized[:, :, 1].astype(np.float32) * alpha1 +
            frame2_resized[:, :, 1].astype(np.float32) * alpha2
        ).astype(np.uint8)
        
        # Bé€šé“ï¼šå‘å·¦åç§»
        if offset > 0:
            # frame1çš„Bé€šé“å‘å·¦åç§»
            blended[:, :-offset, 0] = (
                frame1_resized[:, offset:, 0].astype(np.float32) * alpha1 +
                frame2_resized[:, :-offset, 0].astype(np.float32) * alpha2
            ).astype(np.uint8)
            blended[:, -offset:, 0] = (
                frame1_resized[:, -offset:, 0].astype(np.float32) * alpha1 +
                frame2_resized[:, -offset:, 0].astype(np.float32) * alpha2
            ).astype(np.uint8)
        else:
            blended[:, :, 0] = (
                frame1_resized[:, :, 0].astype(np.float32) * alpha1 +
                frame2_resized[:, :, 0].astype(np.float32) * alpha2
            ).astype(np.uint8)
        
        return blended
    
    def _blend_frames(self, frame1, frame2, progress):
        """ä½¿ç”¨OpenCVåˆæˆä¸¤å¸§"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(frame1, torch.Tensor):
            frame1 = frame1.cpu().numpy()
        if isinstance(frame2, torch.Tensor):
            frame2 = frame2.cpu().numpy()
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        if frame1.dtype == np.float32 or frame1.dtype == np.float64:
            frame1 = (frame1 * 255).clip(0, 255).astype(np.uint8)
        if frame2.dtype == np.float32 or frame2.dtype == np.float64:
            frame2 = (frame2 * 255).clip(0, 255).astype(np.uint8)
        
        # ç¡®ä¿å°ºå¯¸ä¸€è‡´
        if frame1.shape != frame2.shape:
            frame2 = cv2.resize(frame2, (frame1.shape[1], frame1.shape[0]))
        
        # é€æ˜å åŒ–: frame1é€æ¸æ·¡å‡ºï¼Œframe2é€æ¸æ·¡å…¥
        alpha1 = 1.0 - progress  # frame1çš„é€æ˜åº¦
        alpha2 = progress        # frame2çš„é€æ˜åº¦
        
        # ä½¿ç”¨OpenCVçš„addWeightedè¿›è¡Œé€æ˜å åŒ–
        blended = cv2.addWeighted(frame1, alpha1, frame2, alpha2, 0)
        
        return blended
    
    def _frames_to_tensor(self, frames):
        """å°†å¸§åˆ—è¡¨è½¬æ¢ä¸ºè§†é¢‘tensor"""
        if not frames:
            return torch.zeros((1, 1080, 1920, 3), dtype=torch.float32)
        
        # è½¬æ¢ä¸ºtensor
        frame_tensors = []
        for frame in frames:
            if isinstance(frame, np.ndarray):
                # è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ–
                frame_tensor = torch.from_numpy(frame).float() / 255.0
                frame_tensors.append(frame_tensor)
            else:
                frame_tensors.append(frame)
        
        # å †å æˆè§†é¢‘tensor [B, H, W, C]
        video_tensor = torch.stack(frame_tensors, dim=0)
        
        return video_tensor


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VideoCrossfadeNode": VideoCrossfadeNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoCrossfadeNode": "Video overlay",
}
