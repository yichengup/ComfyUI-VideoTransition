"""
è§†é¢‘æ‰­æ›²è½¬åœºèŠ‚ç‚¹ - OpenCVåƒç´ ä½ç§»ç‰ˆæœ¬ï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ï¼‰
ä¸¤ä¸ªè§†é¢‘ç‰‡æ®µä¹‹é—´çš„æ‰­æ›²è½¬åœºæ•ˆæœï¼Œæ”¯æŒå¤šç§æ‰­æ›²æ¨¡å¼
åŸºäºOpenCV remapå‡½æ•°å®ç°çœŸæ­£çš„åƒç´ ä½ç§»æ‰­æ›²
"""

import torch
import numpy as np
import cv2
import time
from comfy.comfy_types.node_typing import ComfyNodeABC, InputTypeDict, IO


class VideoWarpTransitionNode(ComfyNodeABC):
    """è§†é¢‘æ‰­æ›²è½¬åœº - åŸºäºOpenCVåƒç´ ä½ç§»çš„æ‰­æ›²è½¬åœºï¼ˆæ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆï¼‰"""
    
    CATEGORY = "VideoTransition"
    
    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "video1": (IO.IMAGE,),  # ç¬¬ä¸€ä¸ªè§†é¢‘
                "video2": (IO.IMAGE,),  # ç¬¬äºŒä¸ªè§†é¢‘
                "warp_type": ([
                    "swirl",         # æ—‹æ¶¡æ‰­æ›²
                    "squeeze_h",     # æ°´å¹³æŒ¤å‹
                    "squeeze_v",     # å‚ç›´æŒ¤å‹
                    "liquid",        # æ¶²ä½“æ‰­æ›²
                    "wave",          # æ³¢æµªæ‰­æ›²
                ],),
                "total_frames": (IO.INT, {"default": 60, "min": 4, "max": 300}),
                "fps": (IO.INT, {"default": 30, "min": 15, "max": 60}),
            },
            "optional": {
                "warp_intensity": (IO.FLOAT, {"default": 0.5, "min": 0.1, "max": 2.0}),
                "warp_speed": (IO.FLOAT, {"default": 1.0, "min": 0.1, "max": 3.0}),
                "max_scale": (IO.FLOAT, {"default": 1.3, "min": 1.0, "max": 3.0, "step": 0.1}),
                "scale_recovery": (IO.BOOLEAN, {"default": True}),
                "use_gpu": (IO.BOOLEAN, {"default": False}),
                "batch_size": (IO.INT, {"default": 5, "min": 1, "max": 20}),
                "background_color": (IO.STRING, {"default": "#000000"}),
                "width": (IO.INT, {"default": 640, "min": 320, "max": 1920}),
                "height": (IO.INT, {"default": 640, "min": 240, "max": 1080}),
            }
        }
    
    RETURN_TYPES = (IO.IMAGE,)
    RETURN_NAMES = ("video",)
    FUNCTION = "generate_warp_transition"
    
    async def generate_warp_transition(
        self, 
        video1, 
        video2,
        warp_type,
        total_frames, 
        fps,
        warp_intensity=0.5,
        warp_speed=1.0,
        max_scale=1.3,
        scale_recovery=True,
        use_gpu=False,
        batch_size=5,
        background_color="#000000",
        width=640,
        height=640
    ):
        """ç”Ÿæˆè§†é¢‘æ‰­æ›²è½¬åœºæ•ˆæœ - åŸºäºOpenCVåƒç´ ä½ç§»çš„æ‰¹å¤„ç†ä¼˜åŒ–ç‰ˆæœ¬"""
        
        start_time = time.time()
        
        warp_names = {
            "swirl": "æ—‹æ¶¡æ‰­æ›²", 
            "squeeze_h": "æ°´å¹³æŒ¤å‹",
            "squeeze_v": "å‚ç›´æŒ¤å‹",
            "liquid": "æ¶²ä½“æ‰­æ›²",
            "wave": "æ³¢æµªæ‰­æ›²",
        }
        
        print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘æ‰­æ›²è½¬åœºï¼ˆOpenCVåƒç´ ä½ç§»ï¼‰...")
        print(f"   æ‰­æ›²ç±»å‹: {warp_names.get(warp_type, warp_type)}")
        print(f"   æ‰­æ›²å¼ºåº¦: {warp_intensity}")
        print(f"   æ‰­æ›²é€Ÿåº¦: {warp_speed}")
        print(f"   æœ€å¤§ç¼©æ”¾: {max_scale}x")
        print(f"   ç¼©æ”¾æ¢å¤: {'å¯ç”¨' if scale_recovery else 'ç¦ç”¨'}")
        print(f"   æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # æå–è§†é¢‘å¸§
        frames1 = self._extract_video_frames(video1)
        frames2 = self._extract_video_frames(video2)
        
        print(f"ğŸ“¹ è§†é¢‘1: {len(frames1)}å¸§, è§†é¢‘2: {len(frames2)}å¸§")
        print(f"ğŸï¸ è½¬åœºå¸§æ•°: {total_frames}")
        
        # è§£æèƒŒæ™¯é¢œè‰²
        bg_color_bgr = self._parse_background_color(background_color)
        
        # æ‰¹å¤„ç†æ¸²æŸ“
        print("ğŸ¬ å¼€å§‹æ‰¹å¤„ç†æ¸²æŸ“...")
        render_start = time.time()
        
        output_frames = []
        
        for batch_start in range(0, total_frames, batch_size):
            batch_end = min(batch_start + batch_size, total_frames)
            batch_indices = list(range(batch_start, batch_end))
            
            print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(total_frames-1)//batch_size + 1}: å¸§ {batch_start+1}-{batch_end}")
            
            # æ‰¹å¤„ç†ï¼šä¸€æ¬¡å¤„ç†å¤šä¸ªå¸§
            batch_frames = self._process_batch_opencv(
                frames1, frames2, batch_indices, total_frames, warp_type, 
                warp_intensity, warp_speed, max_scale, scale_recovery, bg_color_bgr, width, height
            )
            
            # ç«‹å³æ·»åŠ åˆ°ç»“æœä¸­ï¼Œé¿å…å†…å­˜ç§¯ç´¯
            output_frames.extend(batch_frames)
            
            # æ‰“å°æ‰¹æ¬¡è¿›åº¦
            batch_progress = (batch_end / total_frames) * 100
            print(f"âœ… æ‰¹æ¬¡å®Œæˆï¼Œæ€»è¿›åº¦: {batch_progress:.1f}%")
        
        render_time = time.time() - render_start
        print(f"âœ… æ‰¹å¤„ç†æ¸²æŸ“å®Œæˆï¼Œè€—æ—¶: {render_time:.2f}ç§’")
        
        # è½¬æ¢ä¸ºtensor
        video_tensor = self._frames_to_tensor(output_frames)
        
        total_time = time.time() - start_time
        print(f"âœ… æ‰­æ›²è½¬åœºå®Œæˆï¼Œè¾“å‡º: {video_tensor.shape}")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸš€ å¹³å‡æ¯å¸§: {total_time/total_frames*1000:.1f}ms")
        
        return (video_tensor,)
    
    def _process_batch_opencv(self, frames1, frames2, batch_indices, total_frames, warp_type, 
                             warp_intensity, warp_speed, max_scale, scale_recovery, bg_color_bgr, width, height):
        """ä½¿ç”¨OpenCVè¿›è¡Œæ‰¹å¤„ç†æ‰­æ›²æ¸²æŸ“"""
        batch_frames = []
        
        for i in batch_indices:
            progress = i / (total_frames - 1) if total_frames > 1 else 0
            
            # è·å–å¯¹åº”çš„å¸§
            if len(frames1) == 1:
                frame1_data = self._tensor_to_numpy(frames1[0])
            else:
                frame1_idx = min(int(progress * (len(frames1) - 1)), len(frames1) - 1)
                frame1_data = self._tensor_to_numpy(frames1[frame1_idx])
            
            if len(frames2) == 1:
                frame2_data = self._tensor_to_numpy(frames2[0])
            else:
                frame2_idx = min(int(progress * (len(frames2) - 1)), len(frames2) - 1)
                frame2_data = self._tensor_to_numpy(frames2[frame2_idx])
            
            # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
            frame1_resized = cv2.resize(frame1_data, (width, height), interpolation=cv2.INTER_AREA)
            frame2_resized = cv2.resize(frame2_data, (width, height), interpolation=cv2.INTER_AREA)
            
            # åº”ç”¨æ‰­æ›²æ•ˆæœ - å‰ªæ˜ é£æ ¼
            # ç¬¬ä¸€ä¸ªè§†é¢‘ï¼šæ‰­æ›²ä½†ä¸ç¼©æ”¾
            frame1_warped = self._apply_warp_effect(frame1_resized, warp_type, warp_intensity, warp_speed, progress, 1.0 - progress, is_first_video=True, max_scale=max_scale, scale_recovery=scale_recovery)
            
            # ç¬¬äºŒä¸ªè§†é¢‘ï¼šåå‘æ‰­æ›² + ç¼©æ”¾æ¢å¤
            frame2_warped = self._apply_warp_effect(frame2_resized, warp_type, warp_intensity, warp_speed, progress, progress, is_first_video=False, max_scale=max_scale, scale_recovery=scale_recovery)
            
            # ç¡®ä¿ä¸¤ä¸ªå¸§çš„å°ºå¯¸ä¸€è‡´
            if frame1_warped.shape != frame2_warped.shape:
                frame2_warped = cv2.resize(frame2_warped, (frame1_warped.shape[1], frame1_warped.shape[0]), interpolation=cv2.INTER_AREA)
            
            # æ··åˆä¸¤ä¸ªæ‰­æ›²åçš„å¸§ - ç¡®ä¿æ— é»‘è‰²èƒŒæ™¯çš„å¹³æ»‘è¿‡æ¸¡
            alpha1, alpha2 = self._calculate_blend_weights(progress)
            
            # ä½¿ç”¨addWeightedè¿›è¡Œæ··åˆï¼Œç¡®ä¿æ— é»‘è‰²èƒŒæ™¯
            # alpha1å’Œalpha2å·²ç»åœ¨_calculate_blend_weightsä¸­ç¡®ä¿æ€»å’Œä¸º1
            blended = cv2.addWeighted(frame1_warped, alpha1, frame2_warped, alpha2, 0)
            
            batch_frames.append(blended)
        
        return batch_frames
    
    def _apply_warp_effect(self, image, warp_type, intensity, speed, progress, alpha, is_first_video=True, max_scale=1.3, scale_recovery=True):
        """åº”ç”¨æ‰­æ›²æ•ˆæœåˆ°å•å¸§å›¾åƒ - å‰ªæ˜ é£æ ¼"""
        height, width = image.shape[:2]
        
        # åˆ›å»ºç½‘æ ¼
        y, x = np.indices((height, width), dtype=np.float32)
        
        # è®¡ç®—æ—¶é—´å› å­
        time_factor = progress * speed * np.pi * 2
        
        # æ ¹æ®æ‰­æ›²ç±»å‹å’Œè§†é¢‘ç±»å‹è®¡ç®—ä½ç§»
        if warp_type == "swirl":
            x_offset, y_offset = self._calculate_swirl_displacement(x, y, width, height, intensity, time_factor, progress, is_first_video)
        elif warp_type == "squeeze_h":
            x_offset, y_offset = self._calculate_squeeze_displacement(x, y, width, height, intensity, time_factor, progress, "horizontal", is_first_video)
        elif warp_type == "squeeze_v":
            x_offset, y_offset = self._calculate_squeeze_displacement(x, y, width, height, intensity, time_factor, progress, "vertical", is_first_video)
        elif warp_type == "liquid":
            x_offset, y_offset = self._calculate_liquid_displacement(x, y, width, height, intensity, time_factor, progress, is_first_video)
        elif warp_type == "wave":
            x_offset, y_offset = self._calculate_wave_displacement(x, y, width, height, intensity, time_factor, progress, is_first_video)
        else:
            x_offset, y_offset = np.zeros_like(x), np.zeros_like(y)
        
        # è®¡ç®—æ–°çš„åæ ‡
        x_new = x + x_offset
        y_new = y + y_offset
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        x_new = np.clip(x_new, 0, width-1)
        y_new = np.clip(y_new, 0, height-1)
        
        # åº”ç”¨æ‰­æ›²
        warped = cv2.remap(image, x_new, y_new, cv2.INTER_CUBIC, borderMode=cv2.BORDER_REFLECT)
        
        # ç¬¬äºŒä¸ªè§†é¢‘ï¼šåº”ç”¨ç¼©æ”¾æ¢å¤æ•ˆæœ
        if not is_first_video and scale_recovery:
            warped = self._apply_scale_recovery(warped, progress, width, height, max_scale)
        
        # åº”ç”¨é€æ˜åº¦
        if alpha < 1.0:
            warped = (warped * alpha).astype(np.uint8)
        
        return warped
    
    def _apply_scale_recovery(self, image, progress, width, height, max_scale=1.3):
        """åº”ç”¨ç¼©æ”¾æ¢å¤æ•ˆæœ - ä½¿ç”¨è£åˆ‡æ–¹æ³•å®ç°ç¬¬äºŒä¸ªè§†é¢‘çš„ç¼©æ”¾æ•ˆæœ"""
        h, w = image.shape[:2]
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼šprogress=0æ—¶æœ€å¤§(max_scale)ï¼Œprogress=1æ—¶æ­£å¸¸(1.0x)
        scale = max_scale - progress * (max_scale - 1.0)
        
        # ç¡®ä¿ç¼©æ”¾å€¼åœ¨åˆç†èŒƒå›´å†…
        scale = max(0.8, min(scale, max_scale))
        
        # å¦‚æœä¸éœ€è¦ç¼©æ”¾ï¼Œç›´æ¥è¿”å›åŸå›¾
        if abs(scale - 1.0) < 0.01:
            return image
        
        # ä½¿ç”¨è£åˆ‡æ–¹æ³•ï¼šä»ä¸­å¿ƒè£åˆ‡å‡ºç¼©æ”¾åçš„åŒºåŸŸ
        # å½“scale > 1æ—¶ï¼Œç›¸å½“äºæ”¾å¤§ï¼ˆè£åˆ‡æ›´å°çš„åŒºåŸŸï¼‰
        # å½“scale < 1æ—¶ï¼Œç›¸å½“äºç¼©å°ï¼ˆè£åˆ‡æ›´å¤§çš„åŒºåŸŸï¼‰
        
        # è®¡ç®—è£åˆ‡åŒºåŸŸçš„å¤§å°ï¼ˆä¸ç¼©æ”¾æ•ˆæœç›¸åï¼‰
        crop_scale = 1.0 / scale
        crop_w = int(w * crop_scale)
        crop_h = int(h * crop_scale)
        
        # ç¡®ä¿è£åˆ‡å°ºå¯¸ä¸è¶…è¿‡åŸå›¾
        crop_w = min(crop_w, w)
        crop_h = min(crop_h, h)
        
        # è®¡ç®—è£åˆ‡èµ·å§‹ä½ç½®ï¼ˆå±…ä¸­è£åˆ‡ï¼‰
        start_x = (w - crop_w) // 2
        start_y = (h - crop_h) // 2
        
        # ç¡®ä¿è£åˆ‡ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
        start_x = max(0, min(start_x, w - crop_w))
        start_y = max(0, min(start_y, h - crop_h))
        
        # è£åˆ‡å›¾åƒ
        cropped = image[start_y:start_y + crop_h, start_x:start_x + crop_w]
        
        # å°†è£åˆ‡åçš„å›¾åƒç¼©æ”¾å›åŸå§‹å°ºå¯¸
        result = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_AREA)
        
        return result
    
    def _calculate_blend_weights(self, progress):
        """è®¡ç®—å åŒ–æƒé‡ - ç¡®ä¿æ— é»‘è‰²èƒŒæ™¯çš„å¹³æ»‘è¿‡æ¸¡"""
        # ä½¿ç”¨å¹³æ»‘çš„Så‹æ›²çº¿ï¼Œç¡®ä¿alphaæ€»å’Œå§‹ç»ˆä¸º1
        # é¿å…å‡ºç°é»‘è‰²èƒŒæ™¯ï¼Œä¿æŒè§†é¢‘å†…å®¹çš„è¿ç»­æ€§
        
        # ç¡®ä¿progressåœ¨æœ‰æ•ˆèŒƒå›´å†…
        progress = max(0.0, min(progress, 1.0))
        
        # ä½¿ç”¨æ›´å¹³æ»‘çš„è¿‡æ¸¡æ›²çº¿ï¼Œé¿å…çªç„¶çš„åˆ‡æ¢
        # ç¬¬ä¸€ä¸ªè§†é¢‘ï¼šä»1.0å¹³æ»‘è¿‡æ¸¡åˆ°0.0
        alpha1 = 1.0 - progress
        
        # ç¬¬äºŒä¸ªè§†é¢‘ï¼šä»0.0å¹³æ»‘è¿‡æ¸¡åˆ°1.0
        alpha2 = progress
        
        # åº”ç”¨Så‹æ›²çº¿è®©è¿‡æ¸¡æ›´è‡ªç„¶
        # ä½¿ç”¨smoothstepå‡½æ•°ï¼š3tÂ² - 2tÂ³
        def smoothstep(t):
            return 3 * t * t - 2 * t * t * t
        
        # å¯¹alpha2åº”ç”¨smoothstepï¼Œè®©è¿‡æ¸¡æ›´è‡ªç„¶
        alpha2 = smoothstep(alpha2)
        alpha1 = 1.0 - alpha2
        
        # ç¡®ä¿æƒé‡åœ¨åˆç†èŒƒå›´å†…ï¼Œå¹¶ä¸”æ€»å’Œä¸º1
        alpha1 = max(0.0, min(alpha1, 1.0))
        alpha2 = max(0.0, min(alpha2, 1.0))
        
        # æœ€ç»ˆç¡®ä¿alphaæ€»å’Œä¸º1ï¼Œé¿å…é»‘è‰²èƒŒæ™¯
        total_alpha = alpha1 + alpha2
        if total_alpha > 0:
            alpha1 = alpha1 / total_alpha
            alpha2 = alpha2 / total_alpha
        
        return alpha1, alpha2
    
    def _calculate_swirl_displacement(self, x, y, width, height, intensity, time_factor, progress, is_first_video=True):
        """è®¡ç®—æ—‹æ¶¡æ‰­æ›²çš„ä½ç§» - å‰ªæ˜ é£æ ¼"""
        center_x = width / 2
        center_y = height / 2
        max_radius = np.sqrt(center_x**2 + center_y**2)
        swirl_intensity = intensity * 2
        
        # è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»å’Œè§’åº¦
        dx = x - center_x
        dy = y - center_y
        distance = np.sqrt(dx**2 + dy**2)
        angle = np.arctan2(dy, dx)
        
        # è®¡ç®—å½±å“å› å­
        influence = np.where(distance > 0, 
                           np.clip(1.0 - distance / max_radius, 0, 1), 0)
        influence = influence * influence * (3 - 2 * influence)
        
        if is_first_video:
            # ç¬¬ä¸€ä¸ªè§†é¢‘ï¼šé€æ¸å¢å¼ºçš„æ‰­æ›²
            rotation_factor = progress
            rotation_direction = 1
        else:
            # ç¬¬äºŒä¸ªè§†é¢‘ï¼šåå‘æ‰­æ›²æ¢å¤
            rotation_factor = 1.0 - progress
            rotation_direction = -1
        
        # è®¡ç®—æ‰­è½¬è§’åº¦
        twist_angle = (distance / max_radius) * swirl_intensity * np.pi * influence * rotation_factor * rotation_direction
        
        # è®¡ç®—æ–°åæ ‡
        cos_theta = np.cos(twist_angle)
        sin_theta = np.sin(twist_angle)
        x_new = center_x + dx * cos_theta - dy * sin_theta
        y_new = center_y + dx * sin_theta + dy * cos_theta
        
        # è®¡ç®—ä½ç§»
        x_offset = x_new - x
        y_offset = y_new - y
        
        return x_offset, y_offset
    
    def _calculate_squeeze_displacement(self, x, y, width, height, intensity, time_factor, progress, direction, is_first_video=True):
        """è®¡ç®—æŒ¤å‹æ‰­æ›²çš„ä½ç§» - å‰ªæ˜ é£æ ¼"""
        if is_first_video:
            # ç¬¬ä¸€ä¸ªè§†é¢‘ï¼šé€æ¸å¢å¼ºçš„æŒ¤å‹
            squeeze_intensity = intensity * progress * 58
        else:
            # ç¬¬äºŒä¸ªè§†é¢‘ï¼šåå‘æŒ¤å‹æ¢å¤
            squeeze_intensity = intensity * (1.0 - progress) * 58
        
        if direction == "horizontal":
            center_x = width / 2
            squeeze = np.sin((x - center_x) / width * np.pi * 3) * squeeze_intensity
            x_offset = squeeze
            y_offset = np.zeros_like(x)
        else:  # vertical
            center_y = height / 2
            squeeze = np.sin((y - center_y) / height * np.pi * 3) * squeeze_intensity
            x_offset = np.zeros_like(x)
            y_offset = squeeze
        
        return x_offset, y_offset
    
    def _calculate_liquid_displacement(self, x, y, width, height, intensity, time_factor, progress, is_first_video=True):
        """è®¡ç®—æ¶²ä½“æ‰­æ›²çš„ä½ç§» - å‰ªæ˜ é£æ ¼"""
        if is_first_video:
            # ç¬¬ä¸€ä¸ªè§†é¢‘ï¼šé€æ¸å¢å¼ºçš„æ¶²ä½“æ•ˆæœ
            liquid_intensity = intensity * progress * 30
        else:
            # ç¬¬äºŒä¸ªè§†é¢‘ï¼šåå‘æ¶²ä½“æ¢å¤
            liquid_intensity = intensity * (1.0 - progress) * 30
        
        # å¤šæ–¹å‘æ³¢åŠ¨
        wave1 = np.sin(x * 0.02 + time_factor) * liquid_intensity
        wave2 = np.cos(y * 0.02 + time_factor * 0.7) * liquid_intensity * 0.8
        
        # æ·»åŠ é¢å¤–çš„æ¶²ä½“æ³¢åŠ¨å±‚
        wave3 = np.sin(x * 0.03 + time_factor * 1.3) * liquid_intensity * 0.4
        wave4 = np.cos(y * 0.03 + time_factor * 0.5) * liquid_intensity * 0.3
        
        x_offset = wave1 + wave3
        y_offset = wave2 + wave4
        
        return x_offset, y_offset
    
    def _calculate_wave_displacement(self, x, y, width, height, intensity, time_factor, progress, is_first_video=True):
        """è®¡ç®—æ³¢æµªæ‰­æ›²çš„ä½ç§» - å‰ªæ˜ é£æ ¼"""
        if is_first_video:
            # ç¬¬ä¸€ä¸ªè§†é¢‘ï¼šé€æ¸å¢å¼ºçš„æ³¢æµª
            wave_intensity = intensity * progress * 40
        else:
            # ç¬¬äºŒä¸ªè§†é¢‘ï¼šåå‘æ³¢æµªæ¢å¤
            wave_intensity = intensity * (1.0 - progress) * 40
        
        # æ³¢æµªå½¢æ‰­æ›²
        wave1 = np.sin(x * 0.03 + time_factor) * wave_intensity
        wave2 = np.sin(x * 0.05 + time_factor * 1.5) * wave_intensity * 0.6
        
        # æ·»åŠ å‚ç›´æ–¹å‘çš„æ³¢æµª
        wave3 = np.sin(y * 0.02 + time_factor * 0.8) * wave_intensity * 0.4
        
        x_offset = wave3  # å‚ç›´æ³¢æµªå½±å“xæ–¹å‘
        y_offset = wave1 + wave2  # æ°´å¹³æ³¢æµªå½±å“yæ–¹å‘
        
        return x_offset, y_offset
    
    def _parse_background_color(self, color_str):
        """è§£æèƒŒæ™¯é¢œè‰²å­—ç¬¦ä¸²ä¸ºBGRæ ¼å¼"""
        try:
            # ç§»é™¤#å·
            if color_str.startswith('#'):
                color_str = color_str[1:]
            
            # è§£æRGB
            if len(color_str) == 6:
                r = int(color_str[0:2], 16)
                g = int(color_str[2:4], 16)
                b = int(color_str[4:6], 16)
            elif len(color_str) == 3:
                r = int(color_str[0] * 2, 16)
                g = int(color_str[1] * 2, 16)
                b = int(color_str[2] * 2, 16)
            else:
                # é»˜è®¤é»‘è‰²
                r, g, b = 0, 0, 0
            
            # è¿”å›BGRæ ¼å¼ï¼ˆOpenCVä½¿ç”¨ï¼‰
            return (b, g, r)
        except:
            # è§£æå¤±è´¥æ—¶è¿”å›é»‘è‰²
            return (0, 0, 0)
    
    def _extract_video_frames(self, video_tensor):
        """ä»è§†é¢‘tensorä¸­æå–å¸§"""
        frames = []
        
        if video_tensor.dim() == 4:
            batch_size = video_tensor.shape[0]
            for i in range(batch_size):
                frame = video_tensor[i]
                frames.append(frame)
        else:
            frames.append(video_tensor)
        
        return frames
    
    def _tensor_to_numpy(self, frame_tensor):
        """å°†tensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if isinstance(frame_tensor, torch.Tensor):
            frame_np = frame_tensor.cpu().numpy()
        else:
            frame_np = frame_tensor
        
        # è½¬æ¢æ•°æ®ç±»å‹
        if frame_np.dtype == np.float32 or frame_np.dtype == np.float64:
            frame_np = (frame_np * 255).clip(0, 255).astype(np.uint8)
        
        return frame_np
    
    
    def _frames_to_tensor(self, frames):
        """å°†å¸§åˆ—è¡¨è½¬æ¢ä¸ºè§†é¢‘tensor"""
        if not frames:
            return torch.zeros((1, 640, 640, 3), dtype=torch.float32)
        
        frame_tensors = []
        for frame in frames:
            if isinstance(frame, np.ndarray):
                frame_tensor = torch.from_numpy(frame).float() / 255.0
                frame_tensors.append(frame_tensor)
            else:
                frame_tensors.append(frame)
        
        video_tensor = torch.stack(frame_tensors, dim=0)
        
        return video_tensor


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VideoWarpTransitionNode": VideoWarpTransitionNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoWarpTransitionNode": "Video Warp Transition",
}
